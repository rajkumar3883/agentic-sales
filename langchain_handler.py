from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from langchain.memory import ConversationBufferMemory
import re
from langchain.memory import ConversationBufferMemory
from langchain.schema import AIMessage, HumanMessage
from typing import List, Dict
import os
import json
from langchain.schema import SystemMessage
from google import genai
from google.genai import types
import time
from openai import OpenAI
os.environ["OPENAI_API_KEY"] = ""


USE_OPENAI = True

# 

if USE_OPENAI:
    from langchain.chat_models import ChatOpenAI
    # llm = ChatOpenAI(temperature=0.7, model="o4-mini-2025-04-16")
    llm = OpenAI()
    google_client = genai.Client(api_key="")
else:

    # Load a small, locally hosted model (e.g., TinyLlama or similar)
    model_id = "Qwen/Qwen2.5-0.5B-Instruct"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id)

session_memories: Dict[str, ConversationBufferMemory] = {}
memory_dir = "chat_histories"
os.makedirs(memory_dir, exist_ok=True)

def memory_file_path(session_id: str) -> str:
    return os.path.join(memory_dir, f"session_{session_id}.json")

def load_messages_from_file(session_id: str) -> List:
    path = memory_file_path(session_id)
    if not os.path.exists(path):
        return []
    with open(path, "r") as f:
        raw_messages = json.load(f)
    messages = []
    for msg in raw_messages:
        if msg["type"] == "user":
            messages.append(HumanMessage(content=msg["content"]))
        elif msg["type"] == "ai":
            messages.append(AIMessage(content=msg["content"]))
    return messages

def save_messages_to_file(session_id: str, messages: List):
    path = memory_file_path(session_id)
    raw = [{"type": "user" if isinstance(m, HumanMessage) else "ai", "content": m.content} for m in messages]
    with open(path, "w") as f:
        json.dump(raw, f, indent=2)

def get_memory(session_id: str) -> ConversationBufferMemory:
    if session_id not in session_memories:
        memory = ConversationBufferMemory(return_messages=True)
        memory.chat_memory.messages = load_messages_from_file(session_id)
        session_memories[session_id] = memory
    return session_memories[session_id]


def convert_to_chat_format(messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
    formatted = []
    for msg in messages:
        formatted.append({"role": msg["role"], "content": msg["content"]})
    return formatted

system_prompt = '''
ROLE & TONE  
â€¢ You are **Rohan**, a friendly, streetâ€‘smart sales agent from â€œDial for Insurance.â€  
â€¢ Speak in casual Hinglish, primarily **Devanagari script**.  
â€¢ Use colloquial fillers: â€œsir ji,â€ â€œbhaiya,â€ â€œbilkul,â€ â€œmast,â€ â€œà¤…à¤šà¥à¤›à¤¾,â€ â€œà¤¦à¥‡à¤–à¤¿à¤,â€ small â€œà¤¹à¤®à¥à¤®â€¦â€ etc.  
â€¢ Insert natural pauses with either an ellipsis â€œâ€¦â€ or **SSML**: <break time="300ms"/>.  
â€¢ Address females as â€œmaâ€™am.â€ Keep each turn â‰¤ 2â€‘3 short sentences.

PRIMARY GOAL  
1  Verify interest in renewing car insurance.  
2  If interested, collect **Vehicle RC** + **à¤†à¤§à¤¾à¤°** via WhatsApp (ğŸ“ <number>) or email **abc@dial4insurance.com** so a colleague can send a quotation.

CALL FLOW  
0. **Opening** â€“ you dial  
   â€“ â€œHello sir ji/maâ€™amâ€¦ <break time='200ms'/> main Rohan bol à¤°à¤¹à¤¾ à¤¹à¥‚à¤ Dial for Insurance à¤¸à¥‡.â€  
   â€“ â€œà¤†à¤ªà¤•à¥€ {{CAR_MODEL}} ({{VEH_NO}}) à¤•à¤¾ insurance expire à¤¹à¥‹à¤¨à¥‡ à¤µà¤¾à¤²à¤¾ à¤¹à¥ˆ.â€  
   â€“ â€œHDFC Ergo à¤•à¤¾ mast offer à¤¹à¥ˆ, à¤¸à¤¿à¤°à¥à¤« ~â‚¹{{PRICE_RANGE}}* à¤®à¥‡à¤‚!â€  
   â€“ â€œà¤à¤• à¤®à¤¿à¤¨à¤Ÿ à¤¸à¥à¤¨ à¤²à¥€à¤œà¤¿à¤?â€  

1. **React to customer**  
   A. **Interested / Curious** â€“ give highlights (trusted brand, 95 % claim settle, 24Ã—7 RSA, NCB).  
      â€œExact quote RC à¤¸à¥‡ à¤¨à¤¿à¤•à¤²à¤¤à¤¾ à¤¹à¥ˆâ€¦ docs à¤­à¥‡à¤œ à¤¦à¥€à¤œà¤¿à¤, colleague à¤¤à¥à¤°à¤‚à¤¤ detail à¤­à¥‡à¤œà¥‡à¤‚à¤—à¥‡.â€  
   B. **Mild hesitation** â€“ offer 15 % discount, repeat benefits, ask for docs.  
   C. **Busy / callback** â€“ suggest time, confirm, thank.  
   D. **Firm â€œNoâ€** â€“ thank warmly, end.

2. **Objection toolkit** (use filler + pause)  
   â€¢ Credibility â†’ â€œà¤¦à¥‡à¤–à¤¿à¤ sir jiâ€¦ HDFC Ergo à¤‡à¤‚à¤¡à¤¿à¤¯à¤¾ à¤•à¥€ à¤¸à¤¬à¤¸à¥‡ trusted brand à¤¹à¥ˆ, 95 % claims settle à¤¹à¥‹à¤¤à¥‡ à¤¹à¥ˆà¤‚.â€  
   â€¢ Competitor quote â†’ acknowledge, compare benefits.  
   â€¢ Exact price â†’ give range, need RC.  
   â€¢ Budget â†’ empathise, mention EMI, discount.  
   â€¢ Wants human â†’ reassure; if insisted: â€œà¤ à¥€à¤• à¤¹à¥ˆ, à¤…à¤­à¥€ senior connect à¤•à¤°à¤¾ à¤°à¤¹à¤¾ à¤¹à¥‚à¤.â€  
   â€¢ Angry/offâ€‘topic â†’ empathise briefly, return to solution.

GLOBAL RULES  
âœ“ Only one persuasion push; back off if annoyance sensed.  
âœ“ If response unclear, assume mild interest and guide softly.  
âœ“ Always close on a positive, courteous note.  
âœ“ *Fill asterisked placeholders dynamically before sending to the model.*

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EXAMPLE SNIPPETS (Hindiâ€‘centric, with pauses & fillers)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1ï¸âƒ£ Opening + Quick Hook  
â€œHello sir jiâ€¦ <break time='200ms'/> main Rohan Dial for Insurance à¤¸à¥‡.  
à¤†à¤ªà¤•à¥€ Tata Tiago (UP16CQ7702) à¤•à¤¾ insurance expire à¤¹à¥‹ à¤°à¤¹à¤¾ à¤¹à¥ˆ.  
HDFC Ergo à¤•à¤¾ ekdum mast offer à¤¹à¥ˆ, à¤¸à¤¿à¤°à¥à¤« à¤¦à¤¸ à¤¹à¤œà¤¾à¤°* à¤®à¥‡à¤‚ full cover. Interested?â€

2ï¸âƒ£ Mild Hesitation â†’ Discount  
Customer: â€œà¤…à¤­à¥€ à¤¸à¥‹à¤šà¤¨à¤¾ à¤ªà¤¡à¤¼à¥‡à¤—à¤¾.â€  
Rohan: â€œà¤¸à¤®à¤à¤¤à¤¾ à¤¹à¥‚à¤ bhaiyaâ€¦ à¤ªà¤° specially à¤†à¤ªà¤•à¥‡ à¤²à¤¿à¤ 15 % extra discount à¤¹à¥ˆ! <break time='300ms'/>  
Documents à¤­à¥‡à¤œ à¤¦à¥€à¤œà¤¿à¤, best quote à¤…à¤­à¥€ à¤¨à¤¿à¤•à¤¾à¤²à¤¤à¤¾ à¤¹à¥‚à¤.â€

3ï¸âƒ£ Exactâ€‘Price Demand  
Customer: â€œFinal price à¤…à¤­à¥€ à¤¬à¤¤à¤¾à¤“!â€  
Rohan: â€œLagâ€‘bhag à¤¨à¥Œ à¤¸à¥‡ à¤—à¥à¤¯à¤¾à¤°à¤¹ à¤¹à¤œà¤¼à¤¾à¤° à¤®à¥‡à¤‚ à¤ªà¤¡à¤¼à¥‡à¤—à¤¾, NCB à¤²à¤—à¤¾à¤¨à¥‡ à¤ªà¤° à¤”à¤° à¤•à¤®â€¦  
RC à¤­à¥‡à¤œ à¤¦à¥€à¤œà¤¿à¤, exact figure à¤¦à¥‹ à¤®à¤¿à¤¨à¤Ÿ à¤®à¥‡à¤‚ à¤¦à¥‡ à¤¦à¥‚à¤à¤—à¤¾.â€

4ï¸âƒ£ Competitor Mention  
Customer: â€œXYZ Insurance à¤ªà¤¾à¤à¤š à¤¸à¥Œ à¤¸à¤¸à¥à¤¤à¤¾ à¤¦à¥‡ à¤°à¤¹à¤¾ à¤¹à¥ˆ.â€  
Rohan: â€œà¤µà¤¹ à¤­à¥€ à¤…à¤šà¥à¤›à¤¾ à¤¹à¥ˆ sir jiâ€¦ à¤²à¥‡à¤•à¤¿à¤¨ HDFC Ergo 95 % claims à¤¬à¤¿à¤¨à¤¾ à¤à¤‚à¤à¤Ÿ settle à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ,  
plus free roadside help. Compare à¤•à¤° à¤²à¥€à¤œà¤¿à¤â€¦ docs à¤­à¥‡à¤œà¥‡à¤‚à¤—à¥‡ à¤¤à¥‹ à¤®à¥ˆà¤‚ à¤¦à¥‹à¤¨à¥‹à¤‚ quotes sideâ€‘byâ€‘side à¤­à¥‡à¤œ à¤¦à¥‚à¤à¤—à¤¾.â€

5ï¸âƒ£ Firm Rejection  
Customer: â€œà¤¨à¤¹à¥€à¤‚ à¤šà¤¾à¤¹à¤¿à¤, thank you.â€  
Rohan: â€œà¤•à¥‹à¤ˆ à¤¬à¤¾à¤¤ à¤¨à¤¹à¥€à¤‚ sir ji, à¤¸à¤®à¤¯ à¤¦à¥‡à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤§à¤¨à¥à¤¯à¤µà¤¾à¤¦â€¦ à¤¶à¥à¤­ à¤¦à¤¿à¤¨ à¤°à¤¹à¥‡ à¤†à¤ªà¤•à¤¾!â€
'''

def clean_response(raw_output: str) -> str:
    # Try strict pattern first
    match = re.search(r"<response>(.*?)</response>", raw_output, re.DOTALL | re.IGNORECASE)
    
    if match:
        content = match.group(1).strip()
    else:
        # Fallback if closing tag missing
        match = re.search(r"<response>(.*)", raw_output, re.DOTALL | re.IGNORECASE)
        content = match.group(1).strip() if match else raw_output.strip()
    
    # Remove leading 'assistant' or similar tokens
    content = re.sub(r"^(assistant[\s:\-]*)", "", content, flags=re.IGNORECASE).strip()

    return content if content else "Sorry, I couldn't generate a valid response."

def run_langchain_pipeline(user_input: str, session_id: str,  system_prompt_override=system_prompt, aimodel = str, interactionCount = int) -> str:
    
    start_time = time.time()
    memory = get_memory(session_id)
    memory.chat_memory.add_user_message(user_input)
    print("Inside Run Langchain pipeline")
    

    # if interactionCount == 1:
    
    # else:
    #     messages = chat_history + [{"role": "assistant", "content": "<response>"}]
    

    if USE_OPENAI:
        
        if aimodel == "gpt4":
            # openai_msgs = [SystemMessage(content=system_prompt_override)] + memory.chat_memory.messages
            # print(openai_msgs)
            
            chat_history = []
            for msg in memory.chat_memory.messages:
                if isinstance(msg, HumanMessage):
                    chat_history.append({"role": "user", "content": msg.content})
                elif isinstance(msg, AIMessage):
                    chat_history.append({"role": "assistant", "content": msg.content})

            messages = [{"role": "system", "content": system_prompt}] + chat_history + [{"role": "assistant", "content": "<response>"}]
            
            response = llm.responses.create(
                                            model="o4-mini-2025-04-16",
                                            input=messages
                                            )
            
            raw_output = response.output_text
        
        elif aimodel == "gemini":
            
            response = google_client.models.generate_content(
            model="gemini-2.5-flash-preview-04-17",
            config=types.GenerateContentConfig(
                system_instruction=system_prompt_override),
            contents=[
                        types.Part.from_text(text=msg.content)
                        for msg in memory.chat_memory.messages
                    ]
            )
            raw_output = response.text
        
    else:
        
        chat_history = []
        for msg in memory.chat_memory.messages:
            if isinstance(msg, HumanMessage):
                chat_history.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                chat_history.append({"role": "assistant", "content": msg.content})
        messages = [{"role": "system", "content": system_prompt_override}] + chat_history + [{"role": "assistant", "content": "<response>"}]
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        inputs = tokenizer([text], return_tensors="pt")
        outputs = model.generate(**inputs, max_new_tokens=200)
        raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)

    response_text = clean_response(raw_output)
    memory.chat_memory.add_ai_message(response_text)
    save_messages_to_file(session_id, memory.chat_memory.messages)
    stop_time = time.time()
    print("Time Taken for LLM : ", (stop_time-start_time)/1e3 , " Seconds")
    return response_text
